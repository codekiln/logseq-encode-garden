date-created:: [[2025-02-22 Sat]]
tags:: [[AI/Agent]], [[LangChain]], [[Workshop]], [[Tutorial]]

- # [How Clay Performs Agent Evaluation](https://www.ai.engineer/summit/2025/schedule/clay-evals)
	- meta info
		- **Date**: February 22, 2025
		- **Time**: 2:30 PM - 3:50 PM
		- **Presenters**: [[Person/Nick Huang]] and Ratch Sujithan
		- **Room**: AWS JFK27 (12 W 39th St) 300/301 - entrance 39th St & 5th Ave, large gold doors, bring ID
		- **Description**: In this workshop, Nick Huang from #LangChain will explain agent evaluation concepts. [[Person/Jeff Barg]] or [[Person/Ratch Sujithan]] 
		  will then demonstrate how Clay performs end-to-end agent tests for Claygent across various prompt categories against a ground truth dataset, and discuss how Clay conducts prompt-level evaluations.
	- ## intro
		- ### chain vs agent
			- agents typically use [[AI/Tool/Calling]]
				- agends determine what order to call them in
		- ### issues with agents
			- poor reliability
			- task ambiguity and missing context
			- too many tools
			- ambiguous tools
			- failing to reason
			- relying to heavily on internal
		- ### mitigations
			- express control flow with both deterministic and agentic logic
			- #### vibe check
				- not gooed enough for prod
				- a lot of people **stayed away from sensitive applications** since this was what most were able to do
		- ### how can we be sure that our performance is getting better over time?
			- ![image.png](../assets/image_1740253916864_0.png)
			- cgpt
				- ## General Evaluation Principles
					- **How do we know that our application is actually getting better over time?**
					- **Version Progression:**
						- v1 → v2: *new prompts* (blue)
						- v2 → v3: *different model* (purple)
						- v3 → v4: *new architecture* (green)
					- **LangChain logo present in the bottom-left corner**
					- **Slide number:** 17
					- **Zoom overlay visible at the top**
						- "This meeting is being recorded."
						- "You are screen sharing."
						- "Not hearing anything? Turn up volume"
						- "Talking: Sydney"
			- as we go from #o1 to #o3-mini how can we know what changes?
			- as we go to a supervisor  model
		- ### test versions of your application against full datasets of examples
			- ![image.png](../assets/image_1740254043253_0.png)
			- cgpt
				- ## General Evaluation Principles
					- **Test versions of your application against full datasets of examples**
					- **Diagram Structure:**
						- **Application**
							- v1 → Dataset (red arrow)
							- v2 → Dataset (blue arrow)
								- *new architecture, new prompt, or new model*
						- **Dataset** (diamond shape) → Evaluators
						- **Evaluators** (green box)
							- 65% Accuracy, 40% Hallucination (red)
							- ...
							- 80% Accuracy, 25% Hallucination (blue)
							- ...
					- **LangChain logo present at the bottom-left corner**
					- **Slide number:** 2
					- **Zoom overlay visible at the top**
						- "This meeting is being recorded."
						- "Not hearing anything? Turn up volume"
		- ### how to define evaluators to calculate useful metrics from your experiments
			- ![image.png](../assets/image_1740254232110_0.png)
				- cgpt
					- ## General Evaluation Principles
						- **Test versions of your application against full datasets of examples**
						- **Diagram Structure:**
							- **Application**
								- v1 → Dataset (red arrow)
								- v2 → Dataset (blue arrow)
									- *new architecture, new prompt, or new model*
							- **Dataset** (diamond shape) → Evaluators
							- **Evaluators** (green box)
								- 65% Accuracy, 40% Hallucination (red)
								- ...
								- 80% Accuracy, 25% Hallucination (blue)
								- ...
						- **LangChain logo present at the bottom-left corner**
						- **Slide number:** 2
						- **Zoom overlay visible at the top**
							- "This meeting is being recorded."
							- "Not hearing anything? Turn up volume"
		- ## **Define evaluators to calculate useful metrics from your experiments**
			- ![image.png](../assets/image_1740254259708_0.png)
			- **Define evaluators to calculate useful metrics from your experiments**
			- **Diagram Structure:**
				- **Example (Blue Box)**
					- Input = x1
					- Reference Output = y1
				- **Run (Red Box)**
					- v1 processes Input = x1
					- Produces Output = y_run
				- **Evaluator (Green Box)**
					- Takes:
						- Input
						- Reference Output
						- Output (from Run)
					- **Calculates metrics!**
			- **LangChain logo present at the bottom-left corner**
			- **Slide number:** 21
			- **Zoom overlay visible at the top**
				- "This meeting is being recorded."
		- ### #Q - is [[LangSmith/Evaluator/Offline]] preferred if you have ground truth labels?
			- kinda, yeah. online tends to be "sketchy"
			- you want to do this
		- Use case
		- ## **Pairwise evaluators directly compare different versions of your application**
			- ![image.png](../assets/image_1740254592413_0.png)
			- **Pairwise evaluators directly compare different versions of your application**
			- **Diagram Structure:**
				- **Application**
					- v1 → Dataset A (red arrow)
					- v2 → Dataset A (blue arrow)
						- *new prompt: provide more detail*
				- **Pairwise Evaluator (Green Box)**
					- **Experiment Output:**
						- "Pikachu is a pokemon."
							- preferred = 0
						- vs
						- "Pikachu is an electric type pokemon that evolves from Pichu and into Raichu"
							- preferred = 1
				- **LLM-as-judge Evaluator**
					- is_correct = True
					- is_correct = True
			- **LangChain logo present at the bottom-left corner**
			- **Slide number:** 24
			- **Zoom overlay visible at the top**
				- "This meeting is being recorded."
		- ## **Performing experiments as you iterate on your application is crucial**
			- ![image.png](../assets/image_1740254695788_0.png)
			- **Diagram Structure:**
				- **Application**
					- v1 → Dataset A (red arrow)
					- v2 → Dataset A (blue arrow)
						- *new prompt*
					- v3 → Dataset A (purple arrow)
						- *different model*
					- v4 → Dataset A (green arrow)
						- *new architecture*
				- **Evaluators**
					- v1: 65% Accuracy, 40% Hallucination (red)
					- v2: 80% Accuracy, 25% Hallucination (blue)
					- v3: 85% Accuracy, 25% Hallucination (purple)
					- v4: 95% Accuracy, 10% Hallucination (green)
			- **LangChain logo present at the bottom-left corner**
			- **Slide number:** 25
		- ## **Feedback from "online" evaluators can help you set up useful automations**
			- ![image.png](../assets/image_1740254810454_0.png)
			- **Diagram Structure:**
				- **Filter Condition (Yellow Box - "When")**
					- accuracy > 50%
					- AND
					- hallucination = false
				- **Actions (Triggered if Condition is Met - "Then")**
					- Add to Dataset (Red Box)
					- Add to Annotation Queue (Red Box)
					- More Online Evals (Green Box)
					- Trigger Webhook (Purple Box)
					- etc.
			- **Slide number:** 31
		- ## **Agents need to be evaluated at different levels of granularity**
			- ![image.png](../assets/image_1740255362712_0.png)
			- **Diagram Structure:**
				- **Question → Query Optimization (Purple Box)**
				- **Retrieve Documents (Blue Box)**
					- Multiple documents retrieved
				- **Grade Documents (Yellow Diamond)**
					- Determines if any document is relevant
				- **If relevant → Generate Answer (Blue Circle)**
					- If no relevant document → Human in the Loop (Stick Figure)
				- **Check for Hallucinations (Green Diamond)**
					- If no hallucinations → Answer
					- If hallucinations → Review Needed
			- **LangChain logo present at the bottom-left corner**
			- **Slide number:** 33
			- **Zoom overlay visible at the top**
				- "This meeting is being recorded."
				- "Not hearing anything? Turn up volume"
				- "Talking: Sydney"
	- ## Expected trajectory can be ordered
		- ![image.png](../assets/image_1740255660446_0.png)
			- **v1 Trajectory (Red Box)**
				- `get_calendar(time="5pm")`
				- `get_calendar(time="7pm")` *(green)*
				- `get_calendar(time="6pm")` *(red)*
				- `schedule_meeting(time="7pm")`
			- **Expected Trajectory (Green Box)**
				- `get_calendar(time="5pm")`
				- `get_calendar(time="6pm")` *(red)*
				- `get_calendar(time="7pm")`
				- `schedule_meeting(time="7pm")`
			- **Key takeaway:**
				- The v1 trajectory executes `get_calendar(time="7pm")` before `get_calendar(time="6pm")`, which is incorrect compared to the expected ordering.
			- **LangChain logo present at the bottom-left corner**
			- **Slide number:** 41
	- ## **Multi-turn conversations can be tested individually, or in series**
		- ![image.png](../assets/image_1740256078101_0.png)
		- **Comparison of Testing Approaches:**
			- **One Conversation (Green Box)**
				- Human: A
					- AI: B
				- Human: C
					- AI: D
				- Human: E
					- AI: F
			- **Each Turn can be tested individually (Red Boxes)**
				- **First box:**
					- Human: A
						- AI: ?
				- **Second box:**
					- Human: A
						- AI: B
					- Human: C
						- AI: ?
				- **Third box:**
					- Human: A
						- AI: B
					- Human: C
						- AI: D
					- Human: E
						- AI: ?
		- **Slide number:** 46
	- ## **Regression Testing: Does performance actually improve with a change?**
		- ![image.png](../assets/image_1740256094705_0.png)
		- **Diagram Structure:**
			- **Application Versions**
				- v1 → Dataset A (red arrow)
				- v2 → Dataset A (blue arrow)
					- *new prompt*
				- v3 → Dataset A (purple arrow)
					- *different model*
				- v4 → Dataset A (green arrow)
					- *new architecture*
			- **Evaluators**
				- v1: 65% Accuracy, 40% Hallucination
				- v2: 80% Accuracy, 25% Hallucination
				- v3: 85% Accuracy, 25% Hallucination
				- v4: 70% Accuracy, 10% Hallucination
			- **Latency Measurements**
				- v1: 5s
				- v2: 10s
				- v3: 20s
				- v4: 5s
		- **Slide number:** 52
	- ## [[Person/Ratch Sujithan]]
		- ### Clay
			- "data marketplace for go to markplace"
			- external providers - ocean.io
			- they are going to focus on "claygent" an agent reserach interface
				- it can augment your dataset
			- go to clay.com and see what they have to offer
			- trusted by 300k gtm teams - openai, airbnb, [[Anthropic]], [[CursorAI]], notion, dropbox
		- ### demo that took 5-7 min to create
			- ## LangChain Demo: LinkedIn Profile Finder
				- **Workspace:** Langchain demo
				- ![image.png](../assets/image_1740256185555_0.png)
				- cgpt
					- **Data Table View:**
						- **Columns:**
							- "Find stargazers on" (GitHub user list)
							- "Num Contributions"
							- "Repos Contributed"
							- "LinkedIn Profile Find"
							- "Response"
						- **Example Users:**
							- `allsayar` (160+ contributions)
							- `JohnShahawy` (3481 contributions, `backstage`, `cotmaker`)
							- `davidtsong`, `salomartin`, `sjwithmore`, etc.
					- **Model:** Claygent -> Argon
					- **Configuration:**
						- **Prompt:**  
						  "Given a person's full name, GitHub username, and GitHub profile link, find their LinkedIn profile URL. Follow these steps to ensure accuracy:"
						- **Inputs:**
							- Name (`T Name`)
							- GitHub Username (`T Username`)
							- GitHub URL (`URL`)
						- **Steps to Execute:**  
						  1. Check the GitHub Profile Directly
					- **Additional UI Elements:**
						- "Compare models" button
						- "Save" button
					- **Zoom overlay visible at the top:**
						- "This meeting is being recorded."
						- "Not hearing anything? Turn up volume"
						- "Talking: Sydney"
			- my #notes
				- use "stargazers" on github integration
				- put langchain github url in there
				- get back stargazers in a spareadsheet
				- extract usernames and github urla
				- use github integration to get names and contributions and number of repos they created
				- i'm a recruiter to find out about them and find their backgrounds
				- there's a linkedin profile finder
				- now you can extract person from linkedin profile
				- now you have country profile, email
				- write personalized messages to them
				- very small snapshot of what clay can do
		- ### how we think of evals
		- ### Logs to Action
			- logs here could be json logs
			- analysis
				- clustering
				- regression
		- ## **Evals: Steering AI Strategy**
			- **Evals: Steering AI Strategy**
			- ![image.png](../assets/image_1740257703536_0.png)
				- **Key Points:**
					- Data-driven feedback loop
					- Continuous Feedback
					- Customer-Centric Insights
					- Strategic Alignment
					- Evolutionary Foundation
		- ## Core Elements of Our Evaluation Framework
			- **Development Evaluations**
				- Validate Functionality
				- Test Integration
				- Support New Use Cases
			- **Observability Evaluations**
				- Monitor Usage Patterns
				- Performance Analysis
				- Driving Strategic Decisions
			- **Key Insight:**
				- Observability insights continuously refine development evaluations in a feedback loop.
		- ## Evals at Clay - **Evaluation Pipeline Overview**
			- image here
				- tbd
			- **Production / Observability Evals**
				- **Tools Used:**
					- Segment
					- LangSmith
				- **Process:**
					- Logs are collected and processed in LangSmith
					- Structured events analyzed (Eval_IDs assigned)
					- Pattern analysis and broader insights derived from multiple evaluations
					- Insights linked to Linear for tracking action items
						- Example: Sprint 1 - Implement Use Case 1
						- Example: Sprint 2 - Implement Use Case 2
			- **Development Evals**
				- **Process:**
					- New functionality added and simulated in development
					- Update test suite:
						- Test Case 1 → Test Use Case 1
						- Test Case 2 → Test Use Case 2
						- Test Case N → Test Use Case N
					- Run CI Pipelines:
						- Smoke Test CI
						- Integration Test CI
					- **GitHub Actions handles CI runs**
					- **Final step: Deploy! 🚀**
		-
-