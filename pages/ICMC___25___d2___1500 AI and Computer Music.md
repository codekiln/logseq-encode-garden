# Topics in Artificial Intelligence and Computer Music
	- ## Summary
		- [[2025-06-10 Tue]]; 1500-1630
		- [[Uni/Northeastern/Campus/Snell/Engineering Center/168]]
		- Six scholars from around the world gather together to discuss several topics related to Artificial Intelligence and Computer Music including AI’s effect on electroacoustic music, live networked performance, music education, the music industry, and the use of sound and auditory techniques to promote healing, reduce stress, and improve overall well-being.
		- Six scholars from around the world gather together to discuss several topics related to Artificial Intelligence and Computer Music including AI’s effect on electroacoustic music, live networked performance, music education, the music industry, and the use of sound and auditory techniques to promote healing, reduce stress, and improve overall well-being.
		- #### DONE [[Person/Li Xiaobing]]
		  collapsed:: true
			- ![Li-Xiaobing---Headshot](https://icmc2025.sites.northeastern.edu/files/2025/04/Li-Xiaobing-Headshot.jpg)
		- #### DONE [[Person/Carlos Arana]]
		  collapsed:: true
			- ![Carlos-Arana-headshot](https://icmc2025.sites.northeastern.edu/files/2025/04/Carlos-Arana-headshot.jpg)
		- #### DONE [[Person/Lamberto Coccioli]]
		  collapsed:: true
			- ![Lamberto Coccioli](https://icmc2025.sites.northeastern.edu/files/2025/04/Lamberto-Coccioli-1.jpg)
		- #### DONE [[Person/Marc Battier]]
		  collapsed:: true
			- ![Marc-Battier---Headshot-sm](https://icmc2025.sites.northeastern.edu/files/2025/04/Marc-Battier-Headshot-sm.jpg)
		- #### TODO [[Person/Kenneth Fields]]
		  collapsed:: true
			- ![Kenneth-Fields---Headshot](https://icmc2025.sites.northeastern.edu/files/2025/04/Kenneth-Fields-Headshot.jpg)
		- #### DONE [[Person/Georg Hajdu]]
		  collapsed:: true
			- ![3---Georg-Hajdu---Headshot-2](https://icmc2025.sites.northeastern.edu/files/2025/04/3-Georg-Hajdu-Headshot-2.jpg)
		- Rébecca Kleinberger
		  collapsed:: true
			- ![Rebecca-Kleinberger](https://icmc2025.sites.northeastern.edu/files/2024/10/Rebecca-Kleinberger.jpg)
		- **Panel Moderator.**** **Rébecca Kleinberger, PhD, is a creative technologist and researcher and is jointly appointed at College of Arts, Media and Design and Khoury College of Computer Sciences. Her work leverages the hidden potential of the voice to create new experiences that span from assistive technology to vocal experiences design, including inner-voice and interspecies interactions. Her research connects various fields, including human-computer interaction, computer science, music technology, digital signal processing, wearable computing, AI, neurology, psychology, and animal-computer interaction. She aims to raise awareness of the richness of the voice beyond words and the potential of using the voice to access the mind. After completing her Ph.D. at the MIT Media Lab on the future of voice technology, she held a postdoctoral fellowship at the McGovern Institute for Brain Research at MIT, developing music-based voice feedback systems for creative and therapeutic applications. She also holds a Master’s in Engineering from École National des Arts et Métiers Paris, a Master’s in Computer Graphics from UCL, and a Master’s in Media Arts and Sciences from MIT.
	- TODO attach #YouTube #Video of robot conductor of AI-composed Opera
	- TODO attach #YouTube #Video
	- 15:29 started taking notes
	- ## [[Person/Lamberto Coccioli]] - Beyond Tools and Agents - The Ontological Paradox of AI in Musical Composition
	  collapsed:: true
		- 15:34 tension between AI and society
		- he mentions [[Person/Martin Heidegger]]
		- ### #Transcript
			- ...basically accepts that AI has to go forward no matter what. It's just an unstoppable development that will happen no matter what. We think we know that we can cope with it, but in reality, Heidegger warned us more than 70 years ago that we are completely blind to the essence of technology, and the essence of technology is a way of revealing the world that is enframing, which basically transforms the whole world that is standing reserved to be exploited. Without entering into the details of Heidegger's philosophy, it's important to understand that there is a way of thinking about AI and the developments of technologies that do not take for granted technological determinism. And of course, you know, in our communities we need to advance our research, and many times, or at least this has been recognized by a number of researchers, in our communities we don't take a lot of time to consider the wider impact of our work.
		- ### [[AI Notes]]
			-
			- ### **00:00 — AI as unstoppable, but Heidegger warned us**
			  
			  > …basically accepts that AI has to go forward no matter what. It’s just an unstoppable development that will happen no matter what.
			  
			  > We think we know that we can cope with it, but in reality, Heidegger warned us more than 70 years ago…
			  
			  > …the essence of technology is a way of revealing the world that is *enframing*, which basically transforms the whole world that is *standing-reserve* to be exploited.
			  
			  > …there is a way of thinking about AI and the developments of technologies that **do not take for granted technological determinism**.
			  
			  > …we don’t take a lot of time to consider the wider impact of our work.
		- talks about Indian cosmology - we share reality with other beings.
		- [[Transcript]]
			- Of course, it lacks a biological body. And in fact, it inhabits algorithms and data centers. So it's a completely different situation. In this sense, a subject without a body, AI is very much like animist entities, like spirits, like spectral beings. So it helps to look at generative AI, in my view, from the point of view of Amerindian cosmologies. The work of one anthropologist in particular is very important in this regard. Viveiros de Castro is a Brazilian anthropologist that looked at and developed a theory called perspectivism that shows us how, in reality, we share with other creatures, with animals, with other beings. We share interiorities, according to Amerindian cosmologies. But we have different exteriorities. So completely the opposite of what scientific naturalists approach would say, where we all share the same material bodies, but we've got, of course, a consciousness as humans, that animals don't have, or maybe not completely as like ours. But if you look at this from an Amazon or Amerindian cosmology perspective, the interiorities are the same. So we share the same interior being, like a soul, almost, a subjectivity. What distinguishes us from the animals or spirits is just the form of our body. But we have the same subjectivity. And so this is the case with AI as well. The line between the tool and the agent, then, becomes incredibly blurred. So we can't really think of AI as a tool, because it can be seen as a person. We don't believe that machines have a soul, of course. But users of generative AI increasingly engage with them as if they are persons. So they give AI agency, even motion, the ability to be creative, intentionality. So the fact that the AI mimics language and art so convincingly makes it very easy to relate to it as if it were another person, as if it shared our own interiority. So this brings us to the paradox. On the one hand, AI, generative AI, is the latest step in a kind of a market-driven instrumentalization of creative work. So it's completely commodified, extracting. 15:40 end
		- [[AI Notes]]
			-
			- ### **15:40 — The ontological paradox: is AI a tool or a person?**
			  
			  > **Slide 1 — Our inherent bias: Framing AI through technological determinism**
				- > Generative AI as the next step in the unstoppable development of technology
				- > We are blind to the essence of technology (Heidegger)
				- > Framing all problems as requiring technological solutions, closing off alternative possibilities (Marcuse)
				- > ML, AI and MIR researchers keen to advance their field, without enough consideration of the wider impact of their work (Born, Barnett, Morreale)
			- > **Slide 2 — The challenge to music ontology: What is the meaning of AI-generated music?**
				- > Radically other form of agency
				- > Subject-like behavior without human embodiment
				- > Nonhuman agent, equivalent to *animist entities*
				- > Generative AI shares *interiority* with humans but differs in *exteriority* (Viveiros de Castro’s perspectivism, Descola)
				- > Blurred boundaries between AI as ‘machine’ and AI as ‘person’
				  
				  > AI as a **subject without a body** — like spirits or spectral beings.
				  
				  > Amerindian cosmologies (Viveiros de Castro): we share interiorities with animals/spirits, differ only in bodies.
				  
				  > From this lens, **AI shares our interiority** (intentionality, creativity), but not our embodiment.
				  
				  > Users relate to AI as if it were a person — blurring tool vs. agent.
				  
				  > ➜ This is the **ontological paradox**.
		- ### 15:40 [[My Notes]]
			- it exploits creativity, but it cannot be controlled by us
			- Human AI co-production is the idea he wants to lead us towards
			- he's talking about restricting training data - european bent?
		- ### 15:39 [[Transcript]]
			- So it's completely commodified, extracting value, and of course for money, for profit, from creative work, following a capitalist logic, quite a narrow one, actually. Efficiency, scale, productivity, speed, of course, speed. But on the other hand, AI models operate through attention mechanisms, high-dimensional vector spaces. And these are very opaque. We have no relationship at all with the architecture of our own cognition, the way that humans understand the world. So in a way, generative AI is created as something that is created and used to serve capitalist goals, but it functions as a kind of unknowable and autonomous entity. It exploits human creativity, but its own creativity cannot be controlled by us. So is the answer to make AI more human, more like us, so we can understand it? Or should we try and protect certain areas of human experience as the spaces of human agency where constraints and meaning can be created without AI? I have kind of summarized this in a slide with these contrasting ontologies. So on the left, you've got the kind of human approach to, say for example, musical creativity, and on the right, generative AI approach to musical creativity. So we know that musical creativity derives meaning precisely from constraints. The struggle with material resistance could be the physical properties of the instruments, the friction that you have while you work in a programming language, the affordances that music software gives you or doesn't, the historical way of tradition, the challenges of composition, all these things are the very substance of making meaning when you are creating art. The capacity that generative AI has for infinite generation destroys this productive tension between constraint and possibilities. Everything is equally possible, and maybe everything is equally meaningless. These are, of course, quite binary representations of the whole thing. So let's imagine them like the extremes of a spectrum, really, and let's present them as such. So if we look at the kind of middle position of the spectrum, which represents the kind of co-creative, co-producing environment where humans and AI collaborate, we've got a slightly more interesting way of looking at the problem. So can we, these contrasting ontologies, can we kind of reconcile them in a kind of hybrid co-creation scenario? This scenario points to an important fact that all social technical systems, including generative AI, are culturally constructed and will eventually be modified and adapted through human behavior. The important thing is to identify which forces drive this cultural construction and eventual modifications and adaptations. How are we leading this only to market forces, to the profit motive? I think that artists, musicians, this community needs to be among the forces driving this. We must keep questioning the purpose of generative AI and the very need for generative AI in our world. One way of making this hybrid human-AI scenario work for us would be, in my view, imposing a restriction on training data, reshaping the very conditions of existence of generative AI by constraining the raw materials available.
		- ### 15:39 — Creative meaning vs. infinite generation: the value of constraint
			- > Generative AI **extracts value** from creative work: speed, efficiency, profit — a narrow **capitalist logic**.
			  > But it operates via **opaque mechanisms** (attention, high-dimensional vectors), **detached from human cognition**.
			  > ➜ AI is built to serve rational economic goals but **behaves like an unknowable, autonomous agent**.
			  > ➜ It exploits human creativity but its own output **can't be constrained or fully understood**.
			  > **Key question:** Should we make AI more human to understand it? Or **protect uniquely human spaces** where meaning is made?
			  > Musical meaning arises from **constraints** — resistance, friction, history, tradition, limitation.
			  ➜ Generative AI's **infinite possibility** erases this productive tension, making everything potentially **equally meaningless**.
			  > These are extremes on a **spectrum**. The **middle zone** — Human-AI co-production — offers potential:
			- Recognizes sociotechnical systems like AI are **culturally constructed**
			- Urges us to **guide this construction**, not leave it solely to **market forces**
			  > ➜ Artists and musicians must **question the purpose** and **necessity** of generative AI.
			  > ➜ One path forward: **limit training data** — reshape the foundations of generative AI by **constraining its raw material**.
		- ### 15:41 — The ontological paradox: AI as commodifier of creativity
			- **Slide — The ontological paradox: Commodification of creativity by an inscrutable entity**
			- AI models have **no relationship at all** with the architecture of human cognition
			- AI is the **product of capitalist logic** and an **epistemic enigma**
			- It serves **hyper-rational goals** but is experienced as an **unknowable entity**
			- It **exploits and automates creativity** but **cannot be controlled**
		- ### 15:42 — A new ontology of music: spectrum from human to AI
			- | Humans | Human-AI Co-production | Generative AI |
			  | ---- | ---- | ---- |
			  | Scarcity | Selective generation | Excess |
			  | Sustained effort | Augmented effort | Automation |
			  | Meaning-making | Interpretive computation | Statistical optimization |
			  | Creative constraints | Curated boundaries | Latent space |
			  | Intentionality | Controlled emergence | Simulation |
			  | Embodiment | Interface materiality | Supposed immateriality |
			  | Living culture | RAG, dynamic datasets | Static datasets |
	- ## [[Person/Carlos Arana]] AI in Music - Pedagogical Approach for Productive and Responsible Use - [[School/Berklee]]
	  collapsed:: true
		- 15:51 [[My Notes]]
			- pedagogical approach
				- apply AI to each element of AI
					- mastering
					- composing
					- mixing
					- *it's fairly comprehensive, I haven't considered this*
				- break down
					- task
					- technology
					- application
					- evaluation
						- utility
						- strengths and weaknesses
			- talks about "music information retrieval" which is the [[Data/Science]]
			- Chord Recognition
			- Source Separation
				- isolate, separate original tracks of mixed recordings
				- lalal.ai
			- he likes the most is evaluation
				- use ... how are we going to use this technology
				- typical issues
					- **bleeding** - traces of one instrument appearing in another stem
					- **fidelity loss** - even when isolated, the instruments might sound dull or unnatural
					- **artifacts** - strange watery sounds
			- automatic mixing systems
			- algorithmic composition
			- magenta studio -
			- Text-to-MIDI www.aiva.ai
			- Text-to-Music - connecting text tokens to audio tokens (spectrograms)
			- I bet this guy's class is fantastic,
				- TODO see if I can take his class ...
			- hit song, talent detection
			- ethics and legal aspects
		- 15:53 [[Transcript]]
		  collapsed:: true
			- 1553 start stage after mixing and mastering the music. So, well, I will go really quick. In my design, you have a student's understanding of representation. This will be very useful for afterwards for defining the different models, different types. Signal processing, missing information retrieval. This is the discipline that extracts, like the analog to data mining with music. They understand the different approaches, starting from signal processing, then using more modern approaches, basic on data. Machine learning, the students understand the basics of machine learning, of deep learning. That is the branch of machine learning that deals with unstructured data. Here, we work on deep learning for music. And we go to specific apps of the type of models from missing information retrieval, tempo, beat detection, co-recognition, and always with the corresponding apps. And the famous pedagogical approach, I will put an example here in source separation. Source separation is the type of models that isolates, that separates the original tracks of a mixing recording. Here, we know about the technology. Here, we know the task. We understand the task, what it's for. I'm sorry. Here, the task. Here, the technology. Here, we go to the apps. You understand there's in the industry many type of apps. And then, we go to what I like the most, that is the evaluation, which is going to be the use. How are we going to use this technology as artists, musicians, composers, songwriters? And for instance, in this case, the evaluation is of source separation. We understand the typical issues that we find there. And knowing this, and knowing the technology, the applications, the commercial apps, and the evaluation based on different aspects, as I'm showing here for this particular model of source separation, we are able to know which is going to be the usage that we will give to this particular model, or any other type of models and apps based on AI. So this is the whole journey. So then, we have processing with music, mixing, and mastering. We go through intelligent music production, as it's called, and the different constraints, automatic mixing systems, AI mastering. And here is how we evaluate different levels of control that I named them, inciting, suggestive, independent. Then, we go to the famous general AI, the complexity of a composition process, classical computer-based models, algorithmic composition. And then, we go to the modern, based on deep learning. This is the original one, CheXO. Yes, because I need to be fast. I'm so sorry. But this is magenta. This is the typical models based on recorded networks. These are outdated nowadays. So now, we work with what we call CheXO MIDI. These are the DAW-based general AI models. Typically, one very common, well-known is AVA. And then, we go to the most famous one. They are called text-to-music or text-to-audio, based on text. The models are trained with connecting text tokens to audio tokens, specifically spectrograms. And the technology that is backing the deep learning technology is based on transformers, also the technology that is backing large language models. So that's why they are so connected. And these are the apps, and this is the evaluation. When you work with MIDI models, you are able to have a lot of control over the input and the output. And the quality is MIDI-ish. It sounds MIDI. But when you work with non-parametric models, with text-to-music, and you don't have control over the input and the output, it's like a large language model, but the quality is kind of outstanding. It's studio-level quality. And then, the aid-driven distribution and consumption recommendation systems that we use, that we have in DSP, digital streaming platforms.
		- ### 15:53 — Overview of AI Music Curriculum & Pedagogical Framework [[AI Notes]]
		  collapsed:: true
			- **Curriculum Focus:** Post-production stage (after mixing/mastering).
			- **Student Learning Trajectory:**
				- Understand *representation* of music.
				- Learn foundations of **signal processing** and **music information retrieval** (analogous to data mining).
				- Introduction to **machine learning**, then **deep learning**—used for handling unstructured data like music.
			-
			- ### Deep Learning Applications in Music
			- Core applications include:
				- **Tempo and beat detection**
				- **Chord recognition**
				- **Source separation**
			- All apps are connected with real-world use cases.
			-
			- ### Pedagogical Example: Source Separation
			- **Task:** Isolating individual tracks from a mixed recording.
			- **Framework:**
				- Understand the task and the technology.
				- Explore commercial applications.
				- **Evaluation Focus:** Determine appropriate use for artists, composers, musicians.
				- Includes awareness of technical limitations and context-based usage decisions.
			-
			- ### Intelligent Music Production & Evaluation Dimensions
			- Covers **mixing**, **mastering**, and **AI-assisted production**.
			- Discusses **automatic mixing systems**, **AI mastering**, and evaluation based on:
				- **Level of control**:
					- *Inciting*
					- *Suggestive*
					- *Independent*
			-
			- ### Generative AI in Composition
			- Shift from:
				- **Traditional algorithmic composition** →
				- **Deep learning-based models** (e.g., Magenta → CheXO → CheXO MIDI)
			- **CheXO MIDI:** DAW-integrated, gives users control via MIDI interface.
			- **Text-to-Music Models:**
				- Use transformer-based architectures (same as LLMs).
				- Translate **text tokens** → **audio tokens (spectrograms)**.
				- Tradeoff:
					- *MIDI models*: high control, lower-quality "MIDI-ish" sound.
					- *Non-parametric models*: lower control, high-quality studio output.
			-
			- ### AI-Driven Distribution & Consumption
			- Discusses recommendation systems in **Digital Streaming Platforms (DSPs)**.
			- Students explore how AI influences how music is discovered and consumed.
		- ### [[AI Notes]] Levels of Control in Music text-to-Music Models
		  collapsed:: true
			- #### Parametric Text-to-MIDI Models
			- **User Control**:
				- High user control over music parameters
				- Full symbolic editability
			- **Audio Quality**:
				- Requires additional rendering for realistic sound
				- Raw output in MIDI-quality
			- #### Non-Parametric Text-to-Music Models
			- **User Control**:
				- Limited or no editing of musical structure
				- Control mainly through text prompts
			- **Audio Quality**:
				- Studio-quality audio generated instantly
				- Ready-to-use mixed tracks
			-
			- ### 15:57 — Summary of Audio Commentary
			- Mentions digital signal processing (DSP) platforms like Discovery Weekly and tagging services like Shazam.
			- Introduces machine learning and deep learning applications in music:
				- Song hit prediction (noted as aspirational/imperfect)
			- Ethical and legal considerations are structured around:
				- **Input Consent** — licensing and copyright
				- **Ownership** — of AI-generated songs
				- **Transparency** — right to know if a song was AI-generated
			- This framework defines the pedagogical approach.
	- ## 16:03 [[Person/Marc Battier]] about knowledge limitations in age of AI
	  collapsed:: true
		- 1600 [[Transcript]]
		  collapsed:: true
			- 1600 ...all the models of cybernetics and automatic composition, somehow, and we tried that with our small computer in 1971. The nature of the field, then, of course, has been transformed. We all observed this here at ICMC. I was really amazed by the amount of AI software used in various pieces until now, and I'm sure it's going to continue all week. On a personal note, I did use, also, AI software to compose part of the piece that was performed on the first night, which was semi-improvised, but semi-written, also, and the written part came out of a program called OpenMusic, that some of you may know, which was developed by Gérard Assayag and Carlos Agón at IRCAM in Paris. Now they've moved on to much more advanced software. So let me go back to the question of knowledge. This morning, before the talk, I asked ChatGPT a simple question. Who was Stockhausen's assistant?
		- 1600 [[AI Notes]]
		  collapsed:: true
			- **Knowledge is cumulative. It also can easily [be] lost.**
			- We struggle to perform music for several years ago because implicit knowledge on how to perform has been lost (it was not recorded/notated in a way that was transmissible across centuries.
			- I would like to touch upon this question.
			- As a side remark, I learned about AI during my early student years in France, in the early 1970s (Lejaren Hiller and cybernetics, Pierre Barbaud, Iannis Xenakis…). The nature of the field has, of course, been transformed in recent years. We all observed here during this ICMC were AI software has been widely used. Even in my own work which was performed on the first night, parts of the score had been computed by OpenMusic, the IRCAM software developed by Gérard Assayag and Carlos Agon.
			- **But today, for this panel, I would like to address another aspect of AI.**
			- So, on a musicological level, AI engines such as ChatGPT are not only unreliable but misleading.
			- It is, I believe, our role as teachers and practitioners of electronic and computer music to help correct that sad situation. AI engines expect to be presented with structured data. Their analysis capability works best on information which is organized so that they can fill their data arrays, compare them and make assumptions and correct deductions.
			- The same situation can be applied to the approach of composition.
			- In the field of electronic and computer music, I always have considered [it] important to gather and structure knowledge so that it can serve others — teachers, students, musicologists, concert organizers, curators... — in an informed way.
		- [[Transcript]] 2
		  collapsed:: true
			- It is, I believe, our role as teachers and practitioners of Electronic and Computer Music to help correct that sad situation. AI and Gene expect to be presented with structured data. Their analysis capability works best on information which is organized so that they can field their data arrays, compare them, and make assumptions and correct deductions. The same situation can be applied to the approach of composition, of course. So in the field of Electronic and Computer Music, I always have considered it important to gather and structure knowledge so that it can serve others, teachers, students, musicologists, concert organizers, curators, and AI and Genes, of course, in an informed way. So in the 1970s, I can go back to my student year, I worked with others in the field of Electronic Music and Computer Music documentation, publishing a bibliography and also a catalog of activities in the specific field of Computer Music. It was published in 1978. Later, in 2007, I founded EMSAN, which stands for Electroacoustic Music Studies Asia Network, and EMSAN aims at documenting the electronic music from East Asia. One of the aspects of this endeavor is to produce a database of musical works produced or composed for East Asian countries and regions. To this end, a team of researchers was formed back in the early 2012 to 2010. In fact, Ken Field, who is in Beijing, as I mentioned before, is one member. There are EMSAN members in all countries covered by the EMSAN project. So this is a list of the EMSAN co-editors in various countries and regions. The first database was programmed in 2012 at Sorbonne University. Members from all around the world could enter or edit data or records from anywhere in an instant. It was very fast. However, during my stay at Shenzhen University, I came back last year, I used some of my research funds to hire a programmer for six months and redesign the database between 2023-2024. So this is the address of the database, and this is the front page, and this is how it looks. It's multilingual, it's structured, and it can easily be fed into AI engines. So for this project, I called composers from East Asia to contact me so that we can enter their works in the database. Then we can provide AI engines with data which is multilingual, which is properly curated, and which is structured. Okay, and I think I will skip the rest and say thank you very much.
		- [[AI Notes]] 2
		  collapsed:: true
			- ### Role of teachers, EMSAN, and structured data for AI
			- It is the role of teachers and practitioners of Electronic and Computer Music to help correct the misinformation problem posed by AI.
			- AI engines function best with structured data; their analysis relies on well-organized inputs to populate data arrays, compare entries, and draw accurate conclusions.
			- The same principle applies to composition—structured approaches support better outcomes.
			- Gathering and structuring knowledge has always been important to serve a broad range of stakeholders: teachers, students, musicologists, concert organizers, curators—and now AI engines too.
			- In the 1970s, as a student, he engaged in documentation efforts within Electronic and Computer Music, publishing a bibliography and a catalog of activities in Computer Music (1978).
			- In 2007, he founded **EMSAN** (Electroacoustic Music Studies Asia Network), focused on documenting East Asian electronic music.
			- EMSAN includes a multilingual database of musical works from East Asian countries and regions. A team was formed around 2010–2012, including Ken Field in Beijing and contributors across all covered countries.
			- The first EMSAN database was created in 2012 at Sorbonne University, allowing global, real-time editing.
			- During his stay at Shenzhen University (returned last year), he used research funds to redesign the database (2023–2024) with a new programmer.
			- The updated EMSAN database is multilingual, structured, and optimized for integration with AI engines.
			- He encourages East Asian composers to contribute their work to ensure AI can be trained on well-curated, multilingual, structured data.
			- Concludes the talk by noting he will skip the remainder and expressing thanks.
		-
		-
	- ## [[Person/Georg Hajdu]] Healing Soundscapes
	  collapsed:: true
		- ### Intro slide and meta info
		  collapsed:: true
			- **Title**: Healing Soundscapes
			  
			  *The use of Artificial Intelligence in Musical Soundscape Interventions in the Health-care Sector*
			- **Panel**:
			  
			  CCOM Panel: Topics in Artificial Intelligence and Computer Music
			- **Conference**:
			  
			  ICMC 2025 • Boston • June 10, 2025
			- **Presenter**:
			  
			  Georg Hajdu | Ligeti Center Hamburg
			- **Affiliations (logos shown)**:
			- ligeti center
			- HAW Hamburg
			- TONALi
			- UK?
			- Innovationsfonds Hamburg
			- [Others unclear due to resolution]
			- **Logo**: Healing Soundscapes (abstract circle with quadrants in blue, red, yellow, green)
		- ### [[My Notes]]
		  collapsed:: true
			- Based on Gestalt Psychology
			- he's playing some relaxing music
			- 16:14 the "Bouba kiki" effect
			- he plays a source sound gong and talks about "boubification" -
			- "neutral music" is not intentional - you don't want to attract the attention of the listener.
			- "biang" is 58 strokes it's chinese for noodle
				- it's "so good" it takes so many strokes
			- we are talking about music for waiting areas, a la [[Person/Brian Eno/Music for Airports]]
			- 16:18 he talkes about [[Person/Clarence Barlow]] and he will talk about this more on [[2025-06-14 Sat]]
				- TODO find [[Person/Georg Hajdu]]'s talk on [[2025-06-14 Sat]]
			- importance of non-repetition, allusion to [[Person/John Cage]]
			- #Cool - 11 or more centers are implementing this!
		- ### 16:11 [[Transcript]]
		  collapsed:: true
			- 16:11 start Sort of a paradigm that could be also point into the future in which we combine different types of artificial intelligence as we know the term artificial intelligence is mainly used in the sense now of machine learning or deep learning but artificial intelligence is much more than that it goes back to at least 60-70 years of traditions which are partially probabilistic or we have other other types expert systems for instance that were that also should be referred to as artificial intelligence but let's turn our attention to this particular case what our aim is is to improve the sonic atmosphere in possible waiting areas and what you're hearing right now is music generated by our system and I will expand on that a little bit first of all so why do we do this why do we try to improve the atmosphere it's it's about waiting times you know generally you know typically people sometimes spend five to eight hours in a hospital waiting areas this could be very nerdy people are stressed the areas are very noisy so we need to come up with something that counters the stress and so and we also have to create something that it feels to the majority of people so rock music folk music classical music of course something that people identify with is not ideal so we create we use an engine that's style agnostic and create something that we call neutral music important in this context is a concept is a concept by Gerhard Boehner about the atmosphere as being an in-between phenomena that has an object pole in a subject pole we base our approach on Gestalt psychology and I'm going to talk a little bit about Gestalt psychology it was started like sometime in the 19th century by Austrian philosopher Christian von Ehrenfels and the main proponents of Gestalt psychology are Wertheimer, Hoffkamp and Krüller and so so in other words when you talk about well-being and creating sounds do you also have to consider Gestalt qualities and and there's been research in this area by Krüller in the late 1920s and also by Ramacandra and Hubbard in the early part of the 21st century and they have talked about the Bubatiki effect which is quite important in the context of reading finding the right sounds and expressions for what we're after so we consider sounds the quality of sounds that are particularly suited for the environment and they we look at their spectral behavior and
		- ### 16:11 [[AI Notes]]
		- ### 16:14 [[Transcript]]
		  collapsed:: true
			- 16:14 start Look at their spectral behavior and their behavior in a time domain, and also about their spatial dimensions. And so we came up with a strategy called boobification of the sound. It's like a machine designation of what we're after. We have realized that these sounds have certain qualities which I can classify into two types. The blown bottle and the soft gong that have in the time domain certain qualities, and also the frequency domain. And also what's important is the spatial dimension. We like to have immersive situations, so we work with at least four loudspeakers, sometimes with six in these waiting areas to just create an immersive situation that will envelope the listeners and also the two. So this, for instance, is source sound, and this is the boobified version of that. So what's important when you work in the context of neutral music, you want something that's non-intentional. You don't want to attract the attention of the listener. You want the sonic events to be separate, you want them to be beautiful by itself, and you don't want them to continue wanting to continue towards another event. You don't want them to form melodies that attract your attention. You want, in the Cajun sense, to create events that stand on their own. And so what we're after is, so to speak, the implementation of some of the gestalt qualities of Chinese script. And here you see one of the most complex characters in Chinese script. It's the character from Byung. And Byung Byung is a noodle, a type of noodle, which is so good that you can only describe it with 58 strokes. I mean, fabulous has to be like that. So, and of course, musical references besides Cage are Sanchi and Feldman. You think that would be normal, but Feldman, we all know that. So what is in the waiting area? So the idea is that the soundscapes have to have the capability of integrating all the noisescape around the patients in the waiting area. So we have created this rhombus of the noisescape and the musical soundscape intervention. So on top you have single sounds that have the soothing effect, that stand on their own, that are beautiful themselves, that are well formed. If you use a notion from gestalt psychology, they form a texture of sound. And on the other side, you have the noisescape, which is a combination of different unwanted sounds, but they are as unintentional as the wanted sound themselves. So that's also a very important element, is we have two textures of unintentional sounds. And unintentionality, his voice really driving this whole thing. And together, they form the healing soundscape. And here's the engine that we're using based on work by Clarence Barlow. He started this work in the 1970s. I'll talk a little more about this on Saturday. And so here we have an engine that is based on a few, and yesterday we heard about entanglement. I would say here we have an entangled engine based on probabilities would be perfect for punk and computer music. And it has a kind of a built-in mathematical notions of something like a scale or a meter. And then how these two aspects are entangled to create music that serves our purpose. And so here's what we've done so far. You hear the music being created in real time now. I can now connect with my phone and change some settings. The people in the hospitals, of course, need a very simple interface to interact with. And if this is being used in an operation theater, sometimes I just want to turn that thing off. So what's important now is how do we combine this traditional piece of artificial intelligence with machine learning. So what we did is we asked composers to create presets. And they create five presets. By the way, one of the composers is here, Greg Beathard, sitting there. So we have five presets that define a certain emotional quality. So that's what the composers are asked to do is to create these presets into different corners and to create the term in a neutral point. So these are parameter settings. Now, we're training a neural network to these presets to enable it to perform interpolations between these presets so that we can perform a 24-hour circadian trajectory for every piece. We have nine pieces so far which are intertwined. So one piece, we have a playlist of nine pieces. But when they come to a full circle, they have already moved.
		- ### 16:14 [[AI Notes]]
		- ### Slides - [[AI Notes]]
		  collapsed:: true
			- ### Musical Textures: The Aesthetics of the Disjunct
				- **Melody vs. Healing Sounds**
					- **Melody**
						- Single sounds form the *gestalt* of a melody
						- Analogy to Latin script: characters need to be strung together to form words that bear meaning
					- **Healing Sounds**
						- Every event is *beautiful* in itself
						- No expectation of another event to form a melody
						- Analogy to **Chinese characters**: every character bears meaning
				- **Musical References**
					- Satie, Feldman, Cage ("I don't need sound to talk to me")
				- **Visual**
					- Chinese character: **biáng** (58 strokes)
			- ### The Qualities of the Healing Sound
				- **Two types**
					- “Blown bottle”
					- “Soft gong”
				- **Time domain**
					- Soft attack
					- Fluctuations in sustain sounds (*jitter*)
					- Slow decay
					- Long release
				- **Frequency domain**
					- Slight inharmonicity and noisiness *(see citation)*
					- Low spectral centroid
				- **Spatial perception**
					- Echo, spatial depth, immersion
					- “Oceanic feeling” (Freud)
					- Feeling of being enveloped
				- **Additional**
					- “Bouba-fied” bell sound
				- **Citation**
					- *Marjieh, R., Harrison, P.M.C., Lee, H., et al. Timbral effects on consonance disentangle psychoacoustic mechanisms and suggest perceptual origins for musical scales. Nat Commun 15, 1482 (2024).*
		- ### 16:18 Slide - [[AI Notes]] Musical Textures: Generative Sound Installations
		  collapsed:: true
			- **System**: Stochastic Musical Event Generator
			- **Core Features**:
				- 16 parameters determine the quality of the textures
				- Style-agnostic output
			- **Theoretical Basis**:
				- Built on **formalized musical principles** (scale and meter) developed by **Clarence Barlow** in the 1970s
			- **Tool**: *Autobusk*
				- Developed by Barlow (1986–2000)
				- Further development by Georg Hajdu since 2008
			- **Applications**:
				- Used in **countless real-time and non-real-time** musical settings by Barlow and others
			- **Reference Performance**:
				- *Hajdu*: *Just Her, Jester, Gesture*
				  
				  [https://youtu.be/QSP6_77daes](https://youtu.be/QSP6_77daes)
			- **Visual**:
				- Screenshot of user interface for the generative engine, showing parameters like:
					- Pulse length
					- Event length
					- Meterlicity
					- Harmonicity
					- Melodic cohesion
					- Tonic pitch, pitch range, dynamics, attenuation, etc.
		- ### 16:19 Slide - [[AI Notes]] Healing Soundscapes: Valence/Arousal Space
		  collapsed:: true
			- **Psychological Model**:
				- Emotions are mapped in a **2D valence/arousal space**:
					- **Valence**: positive ↔ negative
					- **Arousal**: high ↔ low
			- **Musical Mapping**:
				- **Composers define 5 key points** in this space:
					- 1 neutral center
					- 4 **extreme presets** (corners)
			- **Behavioral Logic**:
				- The music engine **follows a path** through this emotion space
				- Influenced by a **24-hour circadian rhythm**
			- **AI Role**:
				- **Artificial intelligence interpolates** between the preset points to generate sound dynamically
			- **Visual**:
				- A labeled valence/arousal quadrant plot, showing emotions such as:
					- *Happy, Calm, Angry, Depressed, Excited, Curious*
				- Presets are marked with numbers 1–5, with no.1 at the center (neutral) and 2–5 at the corners
		- ### 16:20 [[Transcript]]
		  collapsed:: true
			- They have already moved on the circadian path and they always sound different, which is an important element because you don't want repetition in something that tires either the personnel or the patients. And so here is an example for the hardware that we've been developing for speakers for the setup in the hospital waiting areas. We're experimenting with a lot of hardware and also with wireless streaming technologies so that these systems could be easily set up. Well, okay, so there's a little conclusion. I'm going to skip that. And this is the building that we're going to be implementing this system. And we're hoping to get some more attention in the future. We're going to outfit about 12 waiting areas with this. And this is our pilot study for that. And we've already developed a system for the central emergency department. So that's it. Thank you.
		- ### 16:20 [[AI Notes]] - Hardware Deployment and Pilot Study
		  collapsed:: true
			- The generative soundscapes **evolve along a circadian path**, ensuring **continuous variation**—important to avoid **listener fatigue** for both patients and staff.
			- **Hardware**:
				- Custom speaker systems are under development.
				- Focus on **wireless streaming** for **easy deployment** in hospital environments.
			- **Pilot Implementation**:
				- Initial deployment targeted at **12 hospital waiting areas**.
				- A **pilot system is already operational** in the **central emergency department**.
			- **Next Steps**:
				- The team aims to draw **more institutional attention** as the project expands.
	- ## 16:22 [[Person/Kenneth Fields]] Chronotechnics: Artifical Time
		- ### [[My Notes]]
			- he's in china right now, it's midnight right now, so this is pre-recorded
			- this is spacey, philosophical, heady
			- normally I'm here for it, but I'm lost.
			- maybe I've hit my threshold for the day.
		-
		-