# What I learned at the [[AI Engineer Summit 2025 NYC Workshops]]
	- ## What *is* the [[AI Engineer Summit]]?
		- ### How I *found out* about the AI Engineer Summit...
			- #### âš¡ flash back to [[@swyx]]'s [[GraphQL]] presentation at [[Conference/React Boston/2018]] ...
			  `babel-blade`: **solving the GraphQL double declaration Problem**
			  ![image.png](../assets/image_1742292361076_0.png){:height 331, :width 397}
			  [swyx ðŸŒ‰ on X - link](https://x.com/swyx/status/1046108066037583872)
			- [[Wayfair]] threw a great party. Got to know [[Person/Shawn @swyx Wang]], who at that time was working at [[Netlify]] as a DevRel.
			- ... Time Passes ...
			- Now my favorite AI podcast is ... [[The Latent Space Podcast]]
			- {{embed ((67d943c3-9827-4274-9daa-b2069d73d4b0))}}
			- {{embed ((67d9437f-1cc1-4fcd-ac88-d804c5ceb2d6))}}
			- {{embed ((67d947ef-69e5-4848-a652-658077c58cfc))}}
		- ### so, *what* is the AI Engineer Summit?
			- ![AI engineer Summit logo](../assets/image_1742369972123_0.png)
			- #### [[AI Engineer Summit NYC 2025]]
			  {{embed ((67d94f45-ee8f-42df-844b-c8a44d9a3cdd))}}
			- #### This team also runs the [[AI Engineer World's Fair]] 
			  ![AI engineer summit logo image](https://do3z7e6uuakno.cloudfront.net/uploads/event/banner/1141926/c3e9a68496cd678944e07c2a05524e5a.jpg){:height 268, :width 543}
		- ### so, *when* was the AI Engineer Summit?
		  * Thu Feb 20 - **Leadership Track**
		  * Fri Feb 21 - **Engineering Track**
		  * Sat Feb 22 - **Workshops**
		      * *I only attended the workshops this year. Would love to attend the full event in the future*
		- ### *where* was the AI Engineer Summit?
		  *Downtown Manhattan, NYC*
		  ![Pic 02](https://lh3.googleusercontent.com/pw/AP1GczORWCrelrJZQ14tww3H_W-jSQOiXjdsJnjooBRAZNoEMYJr-30qT5JTx166v54hfZz_MbAib7m2W2T4KVTwQuJ19rMeNml8IZcZ-Mx8ifeSpm2VZfU=w1920-h1080){:height 309, :width 512}
		  * **Jay Conference Bryant Park** - 109 W 39th St 
		  * **AWS** -  JFK27 (12 W 39th St)
	- ## How I *got* to the AI engineer summit
	  *Many thanks to the leadership team and employer for sponsoring my attendance* 
	  ![Pic 01](https://lh3.googleusercontent.com/pw/AP1GczO5djwdffkqlOOLllHhHi71Dlj_FX7cnZOmR_kfg7TKolWzZwlJoThxwVs-Zzdv89KsXKeRkLrBZH5-m2iVuLKkNuTdhJVL9ADXd4MLjOUNmXkKtjw=w1920-h1080){:height 382, :width 678}
	      * *Port Authority Bus Terminal*
- # Before [[AIES 25 WS 1 - Building Agents with MCP - Mahesh Murag]]
	- ![Pic 01](https://lh3.googleusercontent.com/pw/AP1GczNTS1VKTShXGqSYGMByy6laT8lL5MddZ3bxjDlsMnJcuZzQitD68um-_Yan2Yl5jpBcXgPO_FrqvzxXTStZfIdCGnL86RGzgkNelzm4V-l8dG2ics0=w1920-h1080){:height 389, :width 747}
	- ## Discussions before the first workshop
		- ### 1/3 [[Person/Junrui (Tom) Hu]]
		  Getting coffee before the first workshop, I met Tom, **AI Lead** at [[HanoverPark]], where he's building **AI portfolio management**, **AI inbox analyzer**, real-time fund metrics, and an investor portal. $1.3 billion in assets under administration. Based out of NYC, he was at the AI Engineering track on Friday as well.
		- ### 2/3 - [[Person/Mike Christensen]]
		  ![Mike Christensen](https://cdn.theorg.com/98b0f36b-2fbe-4f45-ab38-1333b975b92d_thumb.jpg){:height 125, :width 111} 
		  ((67da7fc2-1c1f-46e9-84b0-dee824e45395))
		- ### 3/3 - [[Person/Hayden Harrow]]
		  ![Headless Agents logo](https://headlessagents.ai/headless.webp){:height 130, :width 120} 
		  ((67da7fbe-0f7f-45f0-bac4-27dc498b0387))
- # [[AIES 25 WS 1 - Building Agents with MCP - Mahesh Murag]] - #MCP Talk that went viral
	- ## Context - #MCP #Examples from [[David Ondrej - Build Anything with MCP Agents]]
		- ### #Example 1 - using [[GitHub/MCP]] enable [[CursorAI]] to take actions in GitHub
		  {{video https://www.youtube.com/watch?v=TQsP_PlCY1I&t=205s}}
		- ### #Example 2 - enabling [[CursorAI]] to interact with [[Chrome/DevTools]] with [[AgentDeskAI/GitHub/browser-tools-mcp]]
		  {{video https://www.youtube.com/watch?v=TQsP_PlCY1I&t=806s}}
	- ## What problem does [[Model Context Protocol]] solve?
	  ![MCP problem solved](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34123523-7082-4895-b8e4-b26077683c61_1610x868.png){:height 395, :width 718}
		- ### What does this mean ... for [[AI Coding]]? AI Agents in Code editors can now ...
		  * develop technical plans using Jira, Confluence
		  * submit merge requests to GitHub, GitLab, etc
		  * check out a ticket and start working on it
		  * introspect into database, etc
	- ## Side-note - [[Latent Space/Blog/25/03/Why MCP Won]]
		- this talk possibly contributed to MCP going viral in [[2025/03]] 
		  > At current pace, **MCP will overtake OpenAPI in July**
		  ![MCP going viral](https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c23222d-bb03-445b-9806-483eb06c3b75_2554x1640.png)
		- [[Person/Mahesh Murag]]'s talk likely contributed to MCP taking over AI social media feeds
		  ![the twitter hype](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc325ceca-4bdf-44cf-b727-978bb49a89e1_1106x1716.png){:height 551, :width 299}
	- ## The Problem MCP Solves
		- Before MCP, AI application development was fragmented:
		  * Each team developed custom logic (and prompts!) for integrating data, tools, and context into AI apps.
		  * Significant duplication and complexity within companies.
		  * Difficult to reuse components across AI projects.
	- ## Introducing Model Context Protocol (MCP)
		- MCP standardizes interactions between AI apps (clients) and external systems (servers).
		  ![MCP standardized interactions](../assets/image_1740238166470_0.png){:height 358, :width 684}
		- Inspired by [[GraphQL]] and [[Language Server Protocol]].
		  ![Apis -> LSP -> MCP](https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F287e9c38-db9a-4abb-9a88-bc3e3e590355_2252x1214.png)
		- Three interaction types: Tools, Resources, Prompts.
		  ![MCP Deep-Dive](https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe07df610-fa0b-4e18-bca5-6ade934cb64a_2274x1264.png)
	- ## Rapid Adoption of MCP
		- Major companies quickly adopted MCP:
		  * Cloudflare, Shopify, IBM, GitHub, Docker, Replit, and more.
		  * Over **1,100** community-built MCP servers available.
		  * Industry leaders predicting MCP to become standard in AI integration.
		  ![Rapid adoption](https://lh3.googleusercontent.com/pw/AP1GczP4Thkxk-MhVQqm9JdoNSJJnXEiyYdY63F3jsPJVgFoK7Y2HECKVBPS9xXg6sttqCWn-O-4IDyFER4iKyijKOIK0spyAtg4gTrEqxKnHE3xdWqSHjc=w1920-h1080){:height 225, :width 400}
	- ## MCP Components: Tools, Resources, and Prompts
		- ### **Tools** (*Model-controlled*)
		  * LLM knows about the tools it has access to, and uses that to **decide** when to invoke certain actions. These are tiny AI prompts made from a tool schema. Example from [[GitHub MCP]] [here](https://github.com/modelcontextprotocol/servers/blob/main/src/github/index.ts#L65)
			- #### declare the parameters the AI will use to call the `GetFileContents` tool in natural language
			  ```
			  const GetFileContentsSchema = z.object({
			    owner: z.string().describe("Repository owner (username or organization)"),
			    repo: z.string().describe("Repository name"),
			    path: z.string().describe("Path to the file or directory"),
			    branch: z.string().optional().describe("Branch to get contents from"),
			  });
			  ```
			- #### MCP server tools - a list of mappings between descriptions of tools and that tool's implementation in the API
			  ```
			  server.setRequestHandler(..., async () => { return { tools: [ ...{
			    name: "get_file_contents",
			    description: "Get the contents of a file or directory from a GitHub repository",
			    inputSchema: zodToJsonSchema(files.GetFileContentsSchema),
			  }, ...
			  ```
			- #### [[Key Insight]]
			  *AI tools are just a tiny LLM prompt that tells an LLM about something it can do*
			  * "get_file_contents" - supply "Repository owner (username or organization)" and "Repository name" to "Get the contents of a file or directory from a GitHub repository"
		- ### **Resources** (*Application-controlled*)
		  * **shared state** between the [[Agentic System]] and the client application
		    * #Example - in [[ChatGPT/Canvas]], this would be the canvas - both the user and the LLM know about the state of the canvas. 
		  * *NOTE: This is just an example, OpenAI doesn't use MCP for Canvas (yet?)*
		  ![chatgpt canvas](https://images.ctfassets.net/kftzwdyauwt9/uZHfstpnZ78qg2HQhn7m1/25db0387b0f72c0e20c933dcb01533f5/Canvas_Hero.png?w=1080&q=90&fm=webp){:height 155, :width 320}
		- ### **[Prompts](https://modelcontextprotocol.io/docs/concepts/prompts)** (User-controlled):
		  * Predefined, **user-controlled** interactions triggered by users (kinda like slash commands).
		  ![image of prompts](../assets/image_1742381645310_0.png){:height 512, :width 414}
	- ## MCP in Action: Claude AI Integration Demo
		- Claude desktop autonomously:
		  * Identifies and summarizes GitHub issues.
		  * Adds top priority tasks into user's Asana.
		  ![Claude desktop demo](../assets/image_1740239495124_0.png){:height 297, :width 412}
	- [[Anthropic/Blog/24/12/Building Effective Agents]] with #MCP 
	  ![Pic 09](https://lh3.googleusercontent.com/pw/AP1GczOqy2WciToU7rlcJ4dYr_jR2P0z_P8ucVs7oOymOGc1ZNLTefHHPoXLKBrsqMNl39VR_FORrAj2FM9P_hxjY4BoGSMFV6-hHn7cc4LTosQAVNbRBJE=w1920-h1080)
	  * one of the key points of MCP is that the system can **discover new capabilities after the agent system is built**
	  * agent is an "augmented LLM" running in a loop
	  * you can let users of the agent system connect to their data and you as the developer can focus on **the core loop**
	- ## #Example [mcp-agent](https://github.com/lastmile-ai/mcp-agent) from  [[lastmileAI]]
	  * **`mcp-agent`** is a simple, composable framework to **build agents** using #MCP
		- Example: Agent system to do deep research on Quantum computing cybersecurity implications 
		  ![MCP-Agent project structure](https://lh3.googleusercontent.com/pw/AP1GczMwwnFzwCmTQrAIOfOJwG_Hz3G5yPrx4P-1i3rBPwtx1IclV5EHvIjjmp6e7ZWYQWVpFU3dN8DwB_FnBChNN3AMkZytcjAQdmvT4gaKVgKYC_8gx5k=w1920-h1080){:height 313, :width 538}
		  * Uses **sub-agents**: `search_agent`, `fact_checker`, `report_writer`
		  * side note - [[People/Mahesh Murag]] uses [[Windsurf]] here
	- ## MCP Composability & [Sampling](https://modelcontextprotocol.io/docs/concepts/sampling)
	  * Every MCP client can be an MCP Server.
	  * [[MCP/Sampling]] allows **servers to request inference** from clients, ensuring **control of privacy and cost**.
	  ![Hierarchical agent diagram](../assets/image_1740241876883_0.png){:height 225, :width 400}
	- ## MCP Inspector & OAuth Authentication
	  * Inspector provides debugging for MCP connections.
	  * OAuth integration simplifies secure service authentication. Scoped permission boundaries can apply.
	  ![MCP Inspector OAuth](../assets/image_1740242601017_0.png){:height 225, :width 400}
	- ## Autonomous Discovery
	  * Official MCP Registry provides centralized server discovery
	  * Enables agents to find and trust verified MCP servers
	  * Example: Automatic discovery of [[Grafana]] MCP for debugging tasks
	  ![MCP Registry discovery](../assets/image_1740242405280_0.png){:height 225, :width 400}
	- ## Server Discovery
	  * Websites can expose MCP server capabilities via `.well-known/mcp.json`
	  * Enables instant discovery of service integrations
	  * Real example: [[Shopify/MCP]] integration auto-discovery
	  ![Shopify MCP discovery](../assets/image_1740243306243_0.png){:height 225, :width 400}
	- ## #MCP Key [[EdTech]] Takeaways
	  * Of course it helps [[AI Coding]] agents know more about the other systems ([[JIRA]], [[GitHub]], [[PostgreSQL]], etc)
	  * Each [[LMS/Course/Teaching Element]] could express actions AI could take on it as an [[MCP Server]]. Any AI application with permission could interact with that Teaching Element Type
	  * Each LMS could be an [[LMS/Course/Authoring]] MCP Server for AI-enhanced course development
- # [[AI/ES/25/ws/2/GraphRAG - Knowledge Graphs for Agents]]
	- ## Workshop Overview: GraphRAG with Neo4j
	  * Instructor: Alison Cossette, Neo4j Developer Advocate
	  * Goal: Improve AI accuracy using graph-based RAG
	  * Focus: Connecting structured and unstructured data
	  ![Alison Cossette](https://lh3.googleusercontent.com/pw/AP1GczPKe2x5c8T9_9dTrwMgaR12P_ZyEPcoRmY2hB8-eBIrL7O_zvJbKuHarLMGn_tDwc91oJn42uNUKvMS60Of4BF1UJNhDehrpiO8P9qvOXlZ1-bpUpE=w680-h383)
	- ## Traditional RAG Limitations
	  * Struggles with context and temporal information
	  * Can't integrate structured and unstructured data well
	  * Lacks transparency in responses
	  ![External Data Types](https://lh3.googleusercontent.com/pw/AP1GczO8afuu1HdHe5eM28rDd8YEdT7ZORrpyMUUOIXIBxYze4zkfthVgq01cmlX7rGBVduXTji55Hdfr-9bhrblR0LZ5pZR9p_eeDPqQYgEF7NKlQW6cuQ=w680-h383)
	- ## Neo4j Graph Building Blocks
	  * Nodes: Entities (Person, Car)
	  * Relationships: Interactions (KNOWS, LIVES_WITH)
	  * Properties: Node/relationship attributes
	  ![Neo4j Graph Components](https://lh3.googleusercontent.com/pw/AP1GczNT3GiDxoNMcs9lVEFE87hP6FA4KXCPFV0S-nDhWsq6BAM5_YsKf_JRvBVmmezauAUJmFxQ6v_E-ZzeJ-4WDg1SKWXXXixhbDvVkw6FIlh_szrA2Rs=w680-h383)
	- ## GraphRAG Architecture
	  * Domain Graph: Structured knowledge base
	  * Lexical Graph: Document chunk relationships
	  * Memory Graph: User interaction history
	  ![Domain & Lexical Graph](https://lh3.googleusercontent.com/pw/AP1GczOAARYGgrIPeHtn2Gy88OhNMO-T6wBhAMGge-uGZ0fAkF_CnNogRI952c0cjLrybAMMChtEPvflPPmL9wDKbdzUI_VowlZ6uC6cl-U_NBjnBk0wJKU=w680-h383)
	- ## GraphRAG in Action
	  * Converts natural language to Cypher queries
	  * Uses graph embeddings for semantic search
	  * Combines structured and vector search
	  ![GraphRAG Patterns](https://lh3.googleusercontent.com/pw/AP1GczORkXyZklWNW6I-UZdIrOAhjiH8SiSvJML986k_2A6m7M77C02VavV8aUOrU1Bgb3EF9aLfsod9TWJU_nSObOvMaF95lsVW8hI0k8LrqOTcPmNp9J0=w680-h383)
	- ## Key #EdTech Takeaways for GraphRAG
	  * **one of the best ways** to have an AI gradually **develop a "mental model" of a student's abilities** relative to the learning goles; see also [[AI/Digital Twin]] 
	  * possible to take flatfile documents and do entity extraction into a knowledge graph; see also [[EdTech/Idea/LearnMark]] for topic extraction, pre-requisite identification, etc - this would help [[LMS/Course/Authoring]] for custom courses
- ![Pic 03](https://lh3.googleusercontent.com/pw/AP1GczMsj7X800Pr7fExabDh8-Z1tRqBSHzniDKzncp_FqTQr1dPzqDgrGGSU1ON4BlhG7dGaZHGYa_cL_9XQqDiHMVHgwr10Tdbh1UOeY_Wuq_NTZ0nllk=w1920-h1080)
- # [[AI/ES/25/ws/3/How Clay Performs Agent Evaluation]]
	- ## Workshop Overview
	  * Presenters: [[Person/Nick Huang]] ([[LangChain]]), [[Person/Ratch Sujithan]] (ClayCo)
	  * Focus: Best practices in evaluating AI agents at various granularity levels
	  * Key takeaway: Comprehensive testing ensures continuous improvement and reliability
	- ## Part 1 - [[Person/Nick Huang]] from [[LangChain]] on Evaluation
	  * See also [[LangChain/Academy/LangSmith]] course
	  ![Chain vs Agent Issues](https://lh3.googleusercontent.com/pw/AP1GczMaKUGkuv3cOtRkXpq6sXB-IEb8W7wnltne0f4qHpQ9sS-ty-ao7h-Uh4fZHFVvGnOZvIZbPccvnWz_ueFbxJWkIr83PKzexm2lLT6olqO60QSz8WY=w680-h383)
		- ### Why Evaluate Agents?
		  * Task ambiguity
		  * Missing context
		  * Excessive or ambiguous tool usage
		  * Poor reasoning
		  * Solutions include combining deterministic logic with agentic logic for better control flow
		- ### Evaluating Application Versions Over Time
		  * Evaluations track progress through new prompts, models, and architectures
		  * Metrics help verify improvements
		  ![Application Progress](https://lh3.googleusercontent.com/pw/AP1GczOuO9nArnqeYQs4cVrYo86h68jjSgzDZmm_PLNxlFoVSmZ1DBjWTF0adrQAXjndc_RiJmeeH6r0pukA0ROKCUpWekmflE96esfFdfu0USmfJcE9R5k=w680-h383)
		- ### Defining Effective Evaluators
		  * Test agent outputs against ground-truth datasets
		  * Evaluators calculate accuracy, hallucination rates, and other metrics
		  ![Evaluator Setup](https://lh3.googleusercontent.com/pw/AP1GczPIGmnmHU9ePZApyGAwsrREL1qz8KJW6eNiS3_MPz6f2dyiHurJ6r0a50y9IwegCEpjW8Y0UMMqvfIt710y1Jv0rcMxqJgXVJ-M_9U3aVhzZXjkFRY=w680-h383)
		- {{embed ((67ba2c91-d8f5-423c-bb1c-1c0ba5bb07d3))}}
		- ### Pairwise Evaluations
		  * Directly compares two versions of an application side-by-side
		  * Clearly identifies improvements
		  ![Pairwise Comparison](https://lh3.googleusercontent.com/pw/AP1GczOeZAVeKrBWgAeEnklFLQ-UfTg7OvVo-l__eSYEFMqdwE6VuIxrmFZ2RnzKzNcUeUSOQKTuy7_V22Y9wQk6pNw-07ZhRK1oUzN5WcUOmqVLXRtKKKg=w680-h383)
		- ### Feedback Loops from Online Evaluators
		  * Online evaluation enables real-time quality control
		  * Automations triggered by thresholds improve agent performance continuously
		  ![Feedback Automation](https://lh3.googleusercontent.com/pw/AP1GczMQgk2kqx5f7wVpgBh53iw8eSmKGG4WBFenlBY4GnClAxnkazVi0_mSU_5wU4_3KO1lPkrAq3ddvVzb5rLVIge4bjaSmC97tM_gdO-vPnTSUx8U5qI=w680-h383)
		- {{embed ((67ba2c91-d8f5-423c-bb1c-1c0ba5bb07d3))}}
		- ### Granular Evaluations: Breaking Down Agent Tasks
		  * Evaluations performed at different task levels (queries, document retrieval, hallucination checks)
		  ![Granular Eval](https://lh3.googleusercontent.com/pw/AP1GczOoZtHmdnzv65-hxJY2hOrUgR3fCPyvfCQF2Bk9-zFwKuSV8a4R0oWLoC5n7TmJpu2b9eox-vrTptORYN7sa7AlF56L5EuxvpU2T-6Ajha3sWj7TpQ=w680-h383)
		- ### **Multi-turn conversations can be tested individually, or in series**
		  ![image.png](../assets/image_1740256078101_0.png)
		- The structure of how one tests is also determined by the structure of the conversation
		- if your conversation has phases, and you need to make assertions against phases, individual tests for phases may make sense
		- The structure of the golden datasets match the expected structure of how one tests
	- ## Workshop 3, Part 2 [[Person/Ratch Sujithan]] from [[ClayCo]] on their AI Agent evaluation
		- ### ClayCo Demo: LinkedIn Profile Finder
		  ![image.png](../assets/image_1740256185555_0.png){:height 354, :width 542}
		- ### My Notes on this application
		  ![image.png](../assets/image_1740256185555_0.png){:height 354, :width 542}
		  * Sample recruiter app - Given a person's full name, GitHub username, and GitHub profile link, find their LinkedIn profile URL. Find the topics they are interested in using Stargazers on GitHub. Find their backgrounds, connect it to a [[LinkedIn]] profile finder, extract country profile and email, target companies with a certain size and profile, write personalized messages to them 
		  * > "very small snapshot of what clay can do"
		- ### Claygent Evaluation Pipeline
		  * Development Evals: Ensuring quality and stability through CI/CD pipelines
		  * Production/Observability Evals: Analyzing real-world logs to drive strategic decisions
		  ![Eval Pipeline Overview](https://lh3.googleusercontent.com/pw/AP1GczN_vsGXWItowDvl8kflS_2m24O8K_s1EDcy74sifwssY33aDosoorIgmCGMbjjmEa9oXfiZvF4CKf4hZ4oisfDlpoARJ5m2k8eSIaAfRfRlrWYGRh4=w680-h383)
		- ### Development Evaluations: CI/CD Testing
		  * Blackbox E2E smoke tests ensure environmental parity
		  * Integration tests prevent regressions
		  ![Development Testing](https://lh3.googleusercontent.com/pw/AP1GczN37WkSDTlnvBj1zNXaamMq3SgQOSHHgFF1UdKqJVn31WFH3FdKMuHpTVepoxrWQxFlQQcKLZXheXRaRMn-FIVVnJ8nyJL_P4lH_dtNd8KiSYt_K30=w680-h383)
		- ### using [[LangSmith/Evaluator/Online/LLM as a Judge]]: Automated Quality Checks
		  * Judges evaluate responses on **relevance**, **correctness**, and **conciseness**
		  * **Structured scoring system** for clear assessment
		  ![LLM Judging](https://lh3.googleusercontent.com/pw/AP1GczMLenu9I9KrQciFAxPWb3W-iNapPTa9NJZ-edOKw0o0EWSokZzc5-nqkjNzbHkTD8LvHAKcYfv9Ko_HiMMD_sN0YXnELcI-je5_wEiFpmOs7Wyfrs0=w680-h383)
	- ## Workshop 3 - Key Takeaways for [[EdTech]]
	  * [[LangSmith/Evaluator/Offline]] are conducted from [[CICD]] as part of the pull request process
	    * they are powered by a ground truth collection of curated labels in a golden dataset
	  * [[LangSmith/Evaluator/Online]] evaluations are "good enough, quickly" for live-use cases
	  * For **phased conversations**, **each phase should be separately evaluated**
	  * Each part of the application should be tested in isolation
	  * In truly [[Agentic Systems]], you need to also evaluate the [[AI/Tool/Calling]]
- # [[AIES 25 WS 4 â€“ Multi-Agent Workflows with MCP â€“ Dan Mason]]
	- ### Why Multi-Agent Workflows?
		- **"Agents aren't really agents unless they can discover and act beyond the chatbox."**
		- MCP (**Model Context Protocol**) enables agents to interact dynamically with the environment.
		- Allows agents to:
		  * Discover external resources
		  * Interact beyond simple chat interfaces
		  * Execute complex, context-sensitive tasks
		- ![Intro Slide](https://lh3.googleusercontent.com/pw/AP1GczMn93SQDyeKmPaLOr_ENMr8W3TdB2BhcBZzT1cfWErKpvznwI_Qp2AllcqqxWtF1C68GwCZ9W6DJWlbNbM_LSbTI_MrxJW-Eb05aCbrEnBXWaA-0is=w680-h383)
	- ### Building Custom MCP Clients
		- Not limited to Claude's built-in MCP client.
		- Hands-on session demonstrated custom MCP client creation.
		- Framework agnostic but strongly compatible with LangGraph, Autogen, Crew AI.
		- Enables workflows beyond simple code generation.
		- ![Custom MCP Clients](https://lh3.googleusercontent.com/pw/AP1GczPnWIuBRf6ZuYHgFQ1_DXBt39LQpQpG_95nEZmslbkBoQKrQ1umfq_PfdWBC8DgxK52MwCDHiAn2v4goDOJi8D7EXbAiVO6YyEtg7iGlZGBbp16Kks=w680-h383)
	- ### MCP Agentic Workflow Example: CodeAPI
		- CodeAPI: Lightweight FastAPI server for code manipulation.
		- Integrated with LangChain, MCP, and LangGraph.
		- Tools include reading, writing, and modifying files.
		- Used in automated dependency upgrades and refactoring tasks.
		- ![CodeAPI Workflow](https://lh3.googleusercontent.com/pw/AP1GczNfFwj7qIcA0HBsnkXV0Ho8BgeBnKUjr-rFCr9I6b6MkxAR-pew2Gl-h9pOZW5E6BIyY11NF9Y4x0_Jv1RcxMwlOnzmu_h9UGlNmBxF8vj4DdsKkxA=w680-h383)
	- ### AI-Driven Code Upgrades with LangGraph Studio
		- Multi-agent coding workflows illustrated with practical example:
		  * Migrating Java project from JBoss to Spring Boot.
		  * AI autonomously handled dependencies and framework transitions.
		- Human interaction minimized, focused on final verification.
		- Demonstrated efficiency in automated software updates.
		- ![LangGraph Studio Example](https://lh3.googleusercontent.com/pw/AP1GczOjOLs5nwDPyNKKFJBQLKLAzwifP1FN0p505DiZJWZZDmd44345ES4P7_Q7KDVNx-egNl2hB3VXPb9wpEeZk-m9mavVCDjx-LpxmJIZyau_fAzpjQ0=w680-h383)
	- ### Automated Test Generation & Verification
		- Multi-agent orchestration to automate software test creation.
		- Agents collaboratively:
		  * Extract context from codebases
		  * Write and review unit tests
		  * Execute and verify tests autonomously
		- Significantly reduced manual testing efforts.
		- ![Automated Testing Workflow](https://lh3.googleusercontent.com/pw/AP1GczONxbb5c-dt8HTwE1Pqj53wfH9qzzv5C4xL_hDv0fM7oQ9Emu7vemHHC495VWF4D0OEQmB8aLmN1xPEn6liOAqC94Fv3MSf-H6oghddUrnOh9arikc=w680-h383)
	- ### MCP-Enabled Real-Time Tool Discovery
		- Agents dynamically discover available MCP tools at runtime.
		- Example showcased dynamic integration:
		  * Agent performed web search via MCP
		  * Automatically adapted tool usage after an initial access error
		  * Demonstrated robust error-handling and recovery
		- ![Dynamic Tool Discovery](https://lh3.googleusercontent.com/pw/AP1GczPa2cN8B4nWOHbQoJYy5-0EyUKQIJqKcc3aqwX6l2yhllUJzE9ptD6v1pZwN9WpJsFvgHzWvQnlcU4e1LUExw-w51blbTSETJ7Jd_6FcKtZtw5HjoA=w680-h383)
	- ### Real-World Example: Automated PR Generation
		- AI autonomously generated GitHub PRs for feature requests.
		- Multi-agent system:
		  * Planner agent suggested multiple code implementations
		  * Coder agent implemented selected solutions
		  * Orchestrator managed workflow, ensuring safety and control
		- Increased development productivity significantly.
		- ![Automated PR Workflow](https://lh3.googleusercontent.com/pw/AP1GczPdS8cBF-u9pAAS_Tt1u8uWC4bqBhQuezR54qnNfM6xS9Mx-OepLiUVRfFcTt5w6kILPSyA8pzipr3YBZ4hVjUiMZiSh_NyOQT9O1n7pC-OcppHBY8=w680-h383)
	- ### Safety & Security in MCP Multi-Agent Systems
		- Scoped API keys, Docker containers for secure deployments.
		- AI-generated code treated identically to human code for QA.
		- Final deployment always under human oversight.
		- Critical security and compliance safeguards integrated into MCP workflows.
		- ![Security & Safety](https://lh3.googleusercontent.com/pw/AP1GczOVw3IQ8wZ3ZLE6gIbmTDWiyvJcNu5j7bM8IxDxarvhqj8qiGPrz2Iy8cK61IHHhE7MPyfLZfJr4gpIGY6Q2EPOUH5vtf8jQhFcsguWCxzPDH6jklU=w680-h383)
	- ### Final Takeaways from the Workshop
		- MCP standardizes agent integration with diverse tools and services.
		- Multi-agent workflows amplify AI capabilities, enabling complex business logic automation.
		- Safety, security, and flexibility are central to effective multi-agent implementations.
		- Emphasizes human curation of AI-generated solutions rather than direct AI autonomy.
		- ![Final Insights](https://lh3.googleusercontent.com/pw/AP1GczMJmBvHkafLFkxGafgq5Jv90BMtzpycXBw2RcxN9bH1zIDKpS9ZcI2ARz8Kyr9zpEKc0pFIptYERlHv75hMMZiVMdLEp0FADcES4OHbo9WiD8R9cOE=w680-h383)
- ![Pic 04](https://lh3.googleusercontent.com/pw/AP1GczOddYpgPv6uWr9f1zavmUFGB42hHAM0q8Ohi00Y7Jot-octhPyd0mLa2tXpGiqqIjg8Rdxuck7B6ofD7wew6tsU386N-3DenZzYrjqr1v3pFlYjzJc=w1920-h1080)