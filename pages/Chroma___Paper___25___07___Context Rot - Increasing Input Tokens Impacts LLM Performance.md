# [Context Rot: How Increasing Input Tokens Impacts LLM Performance | Chroma Research](https://research.trychroma.com/context-rot)
	- > model performance varies significantly as input length changes, even on simple tasks
	- > models do not use their context uniformly; instead, their performance grows increasingly unreliable as input length grows.
	- covers alternatives to [[AI/Benchmark/Needle-in-a-Haystack]]