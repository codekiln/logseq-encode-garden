- [[EdTech/AI/Pedagogy]]
	- [Inside Praktika's conversational approach to language learning | OpenAI](https://openai.com/index/praktika/)
- \[[AI Coding]]
	- [[Claude Code/Q/Is it possible to see the compaction text]]
	- [[AI/Coding/Ralph Wiggum]]
		- [[YouTube]]
			- with [[Person/Dexter Horthy]]
				- [Ralph Wiggum (and why Claude Code's implementation isn't it) with Geoffrey Huntley and Dexter Horthy - YouTube](https://www.youtube.com/watch?v=O2bBWDoxO4s)
- [[Filed]]
	- [[Shell/Prompt/Concept/PS1 vs PS2]]
- [[Mac/OS/Mission Control]]
	- Noticed a [[Bug]] while in [[CursorAI]] when using split windows in Mac. It seems like the window will not repaint when they are split inside full screen in [[MacOS]]. There's a black line down the center of the page, and I see a portion of my terminal
	- Possibly related to the high cpu warning or memory constraint issues encountered before.
	- I was again reminded of how unsatisfying the [[MacOS]] [[Window/Manager]] is. I really wish I had a window manager that let me create "clusters" of attention, and switch between those clusters. For example, at any one time, I'd love to be able to multitask at "a higher level" - that is, have windows composed for project A and project B, and switch between the layouts for project A and project B.
	- I noticed that [[JetBrains]] plugins/github-copilot-intellij/copilot-agent/native/darwin-arm64/copilot-language-server for [[GitHub/CoPilot]] was taking up almost 3 GB of RAM !?
- [[Langstar]]
	- in [[GitHub/Codespace]] I'm regularly running into high [[CPU]] utilization (100%) with the warning
		- > High codespace CPU (100%) utilization detected. Consider stopping some processes for the best experience.
		- When I ran into this, I was using [[git/worktree]]s. This is possibly related to the out of memory errors that I encountered locally in [[VSCode]] when using a [[DevContainer]]; [[Docker/For Mac]] would completely crash.
		- This morning, after rebuilding the [[Codespace]] for [[Langstar]], upon logging into [[Claude Code]], claude asked me to install the rust [[LSP]] server, and I did. I wonder if that could be part of it?
		- I'm a bit surprised that I'm encountering
	- thinking about progressive disclosure for help commands shipped with [[CLI/Tool]]s.
		- via PR [here](https://github.com/codekiln/langstar/pull/724/changes#r2716746624)
			- Instead of including `--help` as a single flag, I think I'd like to move towards having `langstar prompt help` as a subcommand; that way we can start to support progressive disclosure for AI agents using `langstar`. Running `langstar prompt help` should give the basic commands and serve as the table of contents of the help that's available. Then there should be sub-commands `langstar prompt help detailed-topic-name` which would provide details about `detailed-topic-name`. Here I'm thinking that  this pattern would be repeated at all levels of the hierarchy. We'd start with the prompt sub-command, then roll it out to the other commands. So `langstar prompt list help` would be an overview and equivalent to `langstar prompt list --help`, but there might be `langstar prompt list help details` with details about what the list command can or cannot do. 
			  
			  Also, I'd like to advocate for simplification. What about this, to get rid of duplicate list and search commands?
			- `langstar prompt list` - list all prompts in first page
			- `langstar prompt list --all` list all prompts until last page
			- `langstar prompt list <search-term>` - search for certain prompts, first page
			- `langstar prompt list <search-term> --all` - all results of server side search
			  
			  Similarly, I think we should probably not mirror the names from the API, which are about "commits" and "pulling" etc. That's a pretty unintuitive choice that's irrelevant to non-coders. Instead, I think we should use "CRUD" vocabulary that's aligned with what we use elsewhere across the langstar CLI. In general, I think we've been using the "create, get, update, delete" verbs rather than "read" for get. Please double check me on this.
			  
			  For "Create":
			- `langstar prompt create <prompt-name>`
			  
			  For "Read":
			- `langstar prompt get <prompt-name>` - get the prompt text
			- `langstar prompt get metadata <prompt-name>` - get prompt metadata (when it was saved, etc)
			- `langstar prompt get model <prompt-name>` - get the model information stored with the prompt
			- `langstar prompt get structured-output <prompt-name>` - get the structured output information stored with the prompt
			- `langstar prompt get <prompt-name> --all` - get the prompt text, the metadata, the LLM information, the structured output configuration, everything
			  
			  For "Update" we would update the `get` pattern:
			- `langstar prompt update <prompt-name>` - update prompt text
			- `langstar prompt update metadata <prompt-name> - update metadata for prompt
			- `langstar prompt update structured-output <prompt-name>` - update structured output
			- etc.