- [[LangChain/Ecosystem]]
	- for some reason it's always so hard for me to find [Observability in Studio - Docs by LangChain](https://docs.langchain.com/langsmith/observability-studio#full-example-configuration), which is, to the best of my knowledge, one of the only pieces of documentation which shows how to do this configuration of langgraph assistants using `Annotated[Literal[...]]` and `json_schema_extra`
		- ```python
		  ## Using Pydantic
		  from pydantic import BaseModel, Field
		  from typing import Annotated, Literal
		  
		  class Configuration(BaseModel):
		      """The configuration for the agent."""
		  
		      system_prompt: str = Field(
		          default="You are a helpful AI assistant.",
		          description="The system prompt to use for the agent's interactions. "
		          "This prompt sets the context and behavior for the agent.",
		          json_schema_extra={
		              "langgraph_nodes": ["call_model"],
		              "langgraph_type": "prompt",
		          },
		      )
		  
		      model: Annotated[
		          Literal[
		              "anthropic/claude-sonnet-4-5-20250929",
		              "anthropic/claude-haiku-4-5-20251001",
		              "openai/o1",
		              "openai/gpt-4o-mini",
		              "openai/o1-mini",
		              "openai/o3-mini",
		          ],
		          {"__template_metadata__": {"kind": "llm"}},
		      ] = Field(
		          default="openai/gpt-4o-mini",
		          description="The name of the language model to use for the agent's main interactions. "
		          "Should be in the form: provider/model-name.",
		          json_schema_extra={"langgraph_nodes": ["call_model"]},
		      )
		  
		  ## Using Dataclasses
		  from dataclasses import dataclass, field
		  
		  @dataclass(kw_only=True)
		  class Configuration:
		      """The configuration for the agent."""
		  
		      system_prompt: str = field(
		          default="You are a helpful AI assistant.",
		          metadata={
		              "description": "The system prompt to use for the agent's interactions. "
		              "This prompt sets the context and behavior for the agent.",
		              "json_schema_extra": {"langgraph_nodes": ["call_model"]},
		          },
		      )
		  
		      model: Annotated[str, {"__template_metadata__": {"kind": "llm"}}] = field(
		          default="anthropic/claude-3-5-sonnet-20240620",
		          metadata={
		              "description": "The name of the language model to use for the agent's main interactions. "
		              "Should be in the form: provider/model-name.",
		              "json_schema_extra": {"langgraph_nodes": ["call_model"]},
		          },
		      )
		  ```
- [[DevContainer/Report/25/12/Multi-Env DevContainer Setup]]
	- This morning I was about to start on [üêõ fix(devcontainer): docker-compose.override.yml not picked up by JetBrains Gateway rebuilds ¬∑ Issue #718 ¬∑ codekiln/langstar](https://github.com/codekiln/langstar/issues/718) with the plan to make multiple devcontainer configurations in [[Langstar]], and I opened a devcontainer for [[JetBrains/RustRover]] inside of a [[git/worktree]] that was at `<repo-root>/wip/<branch>`. When I got it open, I had problems with accessing git, which makes sense; there was no [[git/.git]] directory.
		- This made me wonder ... what's the fastest, best, most secure way to spin up a different devcontainer for each [[AI/Coding/Agent]] while keeping them isolated? Sure, git worktrees let you parallelize development, but the ability to do git work in that environment is conditioned on your ability to access the .git directory.
		- But if I really am making a clone from VCS of the entire repo, including the .git dir, and running it in its own unique space with [[Docker]], then why are [[git/worktree]]s even necessary?
		-